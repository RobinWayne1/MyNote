# 零拷贝原理

零拷贝技术的产生就是为了在进行网络IO文件传输时，减少用户态内核态的上下文切换还有减少占用CPU时间的拷贝次数，零拷贝的最终目的就是将占用CPU时间的拷贝次数变成0。不过在看下文时请先注意：一切的优化都是基于用户进程不对从磁盘读出来的数据进行处理为背景的,也就是说用户进程相当于一个数据传输的中介而已。

## 一、传统文件传输

如果没有DMA基础的同学要去看看设备控制器原理。

在用户进程中，传统I/O方式涉及到两个系统调用

```c
read(file, tmp_buf, len);
write(socket, tmp_buf, len);
```

其大致流程如下:



<img src="E:\Typora\MyNote\resources\Java\IO\传统IO方式的传输过程.png" />

1. 用户进程调用`read()`方法,触发用户态→内核态的切换。
2. 内核态负责进行磁盘读取，一般采用DMA技术由设备控制器负责将磁盘中的数据直接读取到内存的内核缓冲区中（更加细分一点的名称就叫`PageCache`）
3. 在读取到内核缓冲区后,CPU负责将`内核缓冲区的数据`拷贝`到用户空间`。==（这里其实就是Linux五种IO模型之一的阻塞IO）==
4. 此时`read()`方法方法返回并且操作系统从内核态切换到了用户态，这个时候用户进程就会调用`write()`方法向`网卡`写入数据
5. 这个时候又会触发用户态→内核态的切换,CPU此时就会将数据从用户空间中写进内核的`sokcet缓冲区里`
6. CPU通知设备控制器进行数据写入,设备控制器进行数据DMA拷贝到网卡中,完成文件传输。

在上述流程中涉及到四次内核态用户态的上下文切换，两次占用CPU时间的拷贝。很显然，这种调用其实有很多步骤是完全不需要的，因为用户进程根本不需要对该文件进行处理或修改。

## 二、`mmap(memory map)+write`

`mmap()`取代了`read()`,减少了一次CPU拷贝。

而同样的，这种方式也涉及了两次系统调用：

```c
buf = mmap(file, len);
write(sockfd, buf, len);
```

大致流程：

![](E:\Typora\MyNote\resources\Java\IO\mmap IO流程.png)

1. 用户进程调用`mmap()`后,设备控制器进行DMA拷贝将数据放到`内核缓冲区`后,应用进程就会与内核共享这块`内核缓冲区`。
2. 在用户进程调用`write()`函数后,CPU就会将==`共享的内核缓冲区`==数据拷贝到==`Socket缓冲区`==
3. 然后CPU通知设备控制器,设备控制器利用DMA拷贝将数据送出

==**mmap的技术要点就是让用户共享了这一块内核缓冲区,所以减少了一次从内核空间到用户空间的CPU拷贝**==

> 后记：了解了一些Linux内核之后再回来说，这种说法并没说错。将这里的内核缓冲区与用户缓冲区的共享看成是虚拟内存行为，真正以物理存储的角度来说其实是同一块（非连续）物理内存。

## 三、`sendfile()`

### Ⅰ、 Linux 内核版本 2.1

在 Linux 内核版本 2.1 中，提供了一个专门发送文件的系统调用函数 `sendfile()`，函数形式如下：

```c
#include <sys/socket.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```

前两个参数分别代表源文件描述符,目标文件描述符,第三个参数代表源端偏移量,第四个参数代表发送的数据的长度。

大致流程如下:

![](E:\Typora\MyNote\resources\Java\IO\sendfile IO流程.png)

`sendfile()`系统调用，可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态，这样就只有 2 次上下文切换，和 3 次数据拷贝。也就是比`mmap()`少了两次系统调用。

### Ⅱ、Linux 内核版本 2.4

如果网卡支持 `SG-DMA`（*The Scatter-Gather Direct Memory Access*）技术（和普通的 DMA 有所不同），那么可以进一步减少通过 CPU 把内核缓冲区里的数据拷贝到 socket 缓冲区的过程。

![](E:\Typora\MyNote\resources\Java\IO\SG-DMA技术下的sendfile()IO过程.png)

**同样是使用`sendfile()`函数,但是由于网卡支持`SG-DMA`,此时就==不再需要将`内核缓冲区`中的数据拷贝到`socket缓冲区`中了==,而是将内核缓冲区中的`fd`和数据长度(其实也就是标明了数据起始地址与传输长度)发送到`socket缓冲区`。支持`SG-DMA`的网卡将从`socket缓冲区`中获得这两个信息,直接从`内核缓冲区`获取数据进行传输。**

上面这段过程就真正实现了零拷贝技术,也就是零CPU拷贝。

## 四、PageCache的概念

上面所提到的内核缓冲区其实就是PageCache，无论应用进程要读取外设上的数据还是将数据写去外设上，都要先经过内存，因为磁盘的读写速度与CPU的读写速度完全不成正比；而又由于只有内核态下的进程才有权限访问外设，所以这块缓冲就设在了内核空间下。

而PageCache有两个作用：==**实现磁盘的预读、作为一个LRU缓存和后写作用**==。

### Ⅰ、磁盘预读

磁盘是怎么读取数据的？磁盘有多个盘片构成，每个盘片有多个圆形磁道，每条磁道又被分为多个扇区，所以在读取数据时，磁头要先找到读取的数据的扇区起点再进行扫描，所以进行物理寻道是非常慢的。如果出现大量的随机IO，磁盘就会不断地移动磁头速度极慢；而如果是顺序IO，磁头只需要继续扫描当前所在扇区就可以获得数据，远远比随机IO要快。

这就是为什么磁盘预读并不会太影响性能。

其二就是磁盘预读的依据。预读意味着每次读取磁盘数据时，每次都多读取一些当前扇区的数据，这依据的是局部性原理。

![](E:\Typora\MyNote\resources\Java\IO\局部性原理.png)

好处就是减少随机IO次数以读取更多所需要的数据。

### Ⅱ、缓存

PageCache缓存与一般的缓存工作机制类似，都是CPU先读cache查看是否有数据，没有再去进行磁盘IO。当然，LRU的依据还是局部性原理。

### Ⅲ、后写

在写操作时需要将修改的页数据写道PageCache，然后根据不同的函数调用以进行不同的刷盘操作。如`write()`函数就是直接将内存的页写道页缓冲,然后由操作系统线程定时执行刷盘操作。这样的好处是将多个小的逻辑写操作合并成大的物理写操作，类似于InnoDB的插入缓冲，这样就能增加顺序写的可能性且减少切换到内核态的次数。

## 五、应用场景

### Ⅰ、大文件传输

大文件传输意味着文件中的数据很低概率会重复使用，所以大文件传输时并不能使用PageCache带来的存储热点数据这个好处，并且由于大文件传输时会占用PageCache时间过长，这会导致其他小文件热点数据无法使用PageCache，加剧降低IO性能。

所以大文件传输场景下并不适合使用零拷贝。

#### 1、阻塞IO式传统文件传输

通过上面的图可以看见，阻塞IO式传统文件传输同样是会用到PageCache，其实是一种更坏的选择。

#### 2、异步IO式传统文件传输

![](E:\Typora\MyNote\resources\Java\IO\异步IO处理大文件传输.png)

> 其实就是普通的Linux五种IO模型之一的异步IO，但是之前那张图我理解的不够清晰，看了大佬的AIO博客勉强知道了，Linux底层对AIO有多个不同的接口实现，`glibc`库是支持非直接IO的异步IO的,而`libaio`则支持直接IO。

可以注意到，这种异步IO直接将数据从磁盘控制器（可以理解成设备控制器）缓冲区拷贝到用户空间，并没有经过PageCache。这种IO就称作直接IO（`Direct IO`）,而经过PageCache的就叫做缓存IO（Buffered IO）。这样就不会阻碍其他小文件热点数据使用PageCache了。

所以大文件传输适合`[异步IO+直接IO]`

### Ⅱ、小文件传输

适合使用零拷贝技术。
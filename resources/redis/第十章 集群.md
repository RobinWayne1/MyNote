# 第十章 集群

当遇到单机内存,并发,流量等瓶颈时,可以采用集群架构方案达到负载均衡的问题.

### 一.数据分布

#### 1.数据分布理论和集群结构

<img src="E:\Typora\resources\redis\虚拟槽分区结构.png" style="zoom:67%;" />

分布式数据库首先要解决把整个数据集按照分区规则映射到多个节点的问题,Redis Cluster**采用虚拟槽分区**,使用分散度良好的哈希函数**把所有数据映射到一个固定范围的整数集合中**,整数定义为槽.槽是集群内数据管理和迁移的基本单位,**每个节点会负责一定数量的槽,而Redis有16384个槽.**

哈希函数根据key的值计算出结果,结果就是对应的槽.

#### 2.集群功能限制

* ==key批量操作支持有限,目前只支持具有相同slot值的key执行mget,mset操作,存在于多个节点上的slot的key则不支持,**存在于相同节点的不同slot也支持.解决办法只能使用hash_tag.**==

* key事务操作支持有限,理由同上
* 不支持多数据库空间
* 复制结构只支持一层,不支持嵌套树状复制结构

### 二.集群搭建

* 集群模式的Redis除了原有的配置文件之外又加了一份集群配置文件,集群内节点信息发生变化,如添加节点,节点下线,故障转移等节点会自动保存集群状态到集群配置文件中,Redis自动维护集群配置文件,不要手动修改.

* 节点ID在集群初始化时只创建一次,节点重启时会加载集群配置文件进行重用,而Redis的运行ID每次重启都会变化.

#### 1.节点握手

(1)集群内节点向集群外节点发送meet消息

(2)集群外节点收到meet后,保存集群内节点信息并恢复pong消息

(3)之后两节点彼此定期通过ping/pong消息进行正常的节点通信

只需要在集群内任意节点上执行`cluster meet`命令加入新节点,握手状态会通过消息在集群内传播,其他节点会自动发现新节点并发起握手流程

#### 2.分配槽

当节点分配了槽才能响应和这些槽关联的键命令

### 三.节点通信

#### 1.通信流程

==**在分布式存储中需要提供维护节点元数据信息的机制,元数据是指:节点负责哪些数据,是否出现故障等状态信息.**==Redis集群采用P2P的Gossip协议,节点彼此不断通信交换信息,一段时间后所有节点都会知道集群完整的信息.

通信过程:

(1)集群中每个节点会开辟一个TCP通道用于节点之间通信

(2)每个节点在固定周期内通过特定规则选择几个节点发送ping消息.

特定规则:

* 每个节点维护定时任务默认每秒执行10次,每秒会随机选取5个节点找出最久没有通信的节点发送ping消息.
* 每 100毫秒都会扫描本地节点列表,如果发现节点最近一次pong消息的时间大于cluster_node_timeout/2,则立刻发送ping消息,防止该节点信息太长时间未更新.

(3)接收到ping消息的节点用pong消息作响应

**解析消息:**

**消息头存放的是自身节点的数据,消息体存放的是集群内其他节点的数据(至少3个,大约为1/10其他节点信息)**

* **解析消息头:消息头包含了发送节点的信息,如果发送节点是新节点且消息时meet类型,将节点信息加入到本地节点列表;如果为已知接单,则更新发送节点的状态,如槽映射关系,主从角色等状态**
* **解析消息体:如果消息体的clusterMsgDataGossip数组包含的节点时新节点,则尝试发起与新节点的meet握手流程;如果是已知节点,则根据clusterMsgDataGossip中的flags字段判断该节点是否下线,用于故障转移**

### 四.集群伸缩

### 1.扩容流程

1. 使用`cluster meet`命令将节点加入集群

2. **迁移原集群中的槽和数据到新集群节点**
* 迁移数据
  
  * (1)目标节点准备导入槽2346
  * (2)源节点准备导出槽2346
     * ==(3)源节点获取槽2346下count数量的键==
  * ==(4)批量迁移count键的数据==
     * (5)循环执行(3)(4)两步

   * 向集群内所有主节点通知槽迁移的信息

收缩就是扩容的相反,理论一致

==如果要忘记某节点,对于主从都下线的情况,建议先下线从节点再下线主节点,防止不必要的全量复制==

### ==⭐五.请求路由(以此为记忆框架)==

**注:**

* ==Lua和事务需要操作的key必须在一个节点上，如果要使用这两个功能只能使用hash_tag来保证数据在同一个节点==

* **使用`set {hash_tags}:tweet:1`,可以使相同的hash_tag具备相同的slot**

#### 1.请求重定向(MOVE重定向)

**在集群模式下,Redis节点接收任何键相关命令时==首先计算key对应的槽==,再根据槽找出所对应的结点(所有集群节点都保有集群槽的拓扑结构,通过节点通信实现),如果结点是自身,才处理命令;==否则则要返回MOVE重定向错误以告诉客户端该key对应的槽在哪个节点==**

#### ==⭐2.Smart客户端==

Smart客户端通过在内部维护slot-->node的映射关系,在**==客户端本地就可以实现键到节点的查找==**,从而保证IO效率最大化(使用`cluster slot`命令来获取槽和节点的映射关系)

==**流程如下:(注意两种错误:连接错误和MOVE重定向错误)**==

1. **计算hash并根据slots缓存映射获取目标节点连接,发送命令**

2. **如果出现连接错误==(连接错误出现可能是因为集群节点宕机)==,则使用随机连接==(即连接随机集群节点,这样做的目的是使得节点报MOVE重定向错误以更新slot缓存,新节点可能是宕机节点的从节点)==重新执行键命令,并使 重试次数参数-1**

3. **捕获到MOVE重定向错误,使用`cluster slot`来更新缓存,之后则根据缓存继续进行重试**

4. **当重试次数到达0,抛出JedisClusterMaxRedirectionsExcetion异常**

#### 3.ASK重定向

Redis在做==集群伸缩==**(故障转移中集群的数据不可用)**的过程中,数据从源节点迁移到目标节点过程中,如果客户端发送命令给源节点

* 如果此时数据已经被迁移到了目标节点,则会返回客户端ASK重定向,然后客户端向目标节点发送asking命令打开客户端连接标识,再执行键命令.如果存在数据则执行,不存在则返回不存在信息
* 如果源节点存在数据则直接返回

==ASK重定向说明集群正在进行slot数据迁移,客户端无法知道什么时候迁移完成,因此只能是临时性的重定向,客户端不会更新slots缓存.但是MOVED重定向说明键对应的槽已经明确指定到新的节点,因此需要更新slots缓存==

##### ①批量操作

正在迁移过程中进行mget操作,如果mget里面的key此时已经不在同一个节点上则会报错

但是使用Pipeline进行逐条get操作时,源节点会对每条get操作返回信息,如果数据在源节点则直接返回,如果已迁移则返回ASK重定向,可以根据重定向信息重新请求目标节点得到数据.所以集群环境下对于使用批量操作的场景,优先使用Pipeline方式

### 六.故障转移

==**⭐故障转移的大体流程:**==

1. 集群中节点会定期向其他节点发送ping消息,**消息中带有本地节点的集群拓扑状态**,接收节点以pong回应,若集群中某节点在`cluster-node-timeout`内一直与另一集群节点通信失败,则更新此节点本地状态为客观下线
2. 下线节点的下线状态通过ping消息在集群中传播,当集群中的 某节点 发现有半数以上的其他节点的ping消息的             下线节点状态都为主观下线,则标记此下线节点状态为客观下线,此时该节点将向整个集群广播下线节点的fail消息
3. 当**下线节点的从节点(从节点全权负责故障转移)**接收到fail消息后,开始准备故障恢复.故障恢复过程将通过集群中各个主节点的投票选举选出替换的从节点,从节点将会替换成主节点

故障发现通过消息传播机制实现的,包括主观下线和客观下线

#### 1.主观下线

流程说明:

(1)节点a发送ping消息给节点b,如果通信正常将接受到pong消息,节点a更新最近一次与节点b的通信时间

(2)如果节点a与节点b通信出现问题则断开连接,下次会进行重连.如果通信一直失败,则节点a记录的与节点b最后通信时间将无法更新

(3)a节点的cron定时任务检测到与节点b最后通信时间超过cluster-node-timeout时,更新本地对节点b的状态为主观下线

#### 2.客观下线

当某个节点判断另一个节点主观下线后,相应的节点状态会跟随消息在集群内传播.当节点在ping/pong消息体中发现含有主观下线的节点状态时,会在本地找到故障节点的ClusterNode结构,保存到下线报告链表中.当==半数以上====持有槽==的主节点都标记某个节点是主观下线时,触发客观下线流程

必须是持有槽的节点原因:集群模式下只有处理槽的主节点才负责读写请求和集群槽等关键信息的维护,**而从节点只进行主节点数据的状态信息的复制**

流程说明:

(1)当消息体内含有其他节点的pfail状态会判断发送节点的状态,如果发送的节点是主节点则对报告的pfail状态处理,从节点则忽略

(2)更新clusterNode内部下线报告链表

```java
//结构
//某主节点
{
	clusterNode//其他节点的信息
	{
		//下线链表
        {
            clusterNodeFailReport//下线报告
            {
                node//报告该节点为主观下线的节点
                time//收到下线报告的时间    
            }
        }
	}
}
```



* 节点内对每个节点clusterNode结构中都会存在一个下线链表结构,保存了其他主节点针对这个节点的下线报告

* 下线报告中保存了报告故障的节点结构和最近收到下线报告的时间
* 每个下线报告都会存在有效期,每次在尝试触发客观下线时都会检测下线报告是否过期(期限:cluster_node_timeout*2),对于过期的下线报告将被删除,**即如果主观下线上报速度赶不上下线报告过期速度,那么故障节点永远无法标记为客观下线**

(3)根据更新后的下线报告链表尝试客观下线

* 若下线报告大于槽主节点数量一半,标记故障节点客观下线并广播fail消息通知所有节点故障节点标记为客观下线

####  ==3.故障恢复==

当从节点通过内部定时任务发现自身复制的主节点进入客观下线时,将会触发故障恢复流程

(1)资格检查

* 如果从节点与主节点断线时间超过cluster-node-time*cluster-slave-validity-factor则从节点不具备故障转移资格

(2)准备选举时间

复制偏移量越大说明从节点延迟越低,则这个从节点越快进行选举.目的是通过延迟选举机制支持优先级.

(3)发起选举

* 更新配置纪元
  * 一个配置纪元代表一个版本,具体看书

* 广播选举消息,并居鲁已发送过消息的状态,保证该从节点在一个配置纪元内只能发起一次选举

(4)选举投票

只有持有槽的主节点才会处理故障选举消息,每隔持有槽的节点在一个配置纪元内都有唯一的一张票,当接到第一个请求投票的从节点消息时就投给他,之后相同配置纪元内其他从节点的选举消息将忽略.当从节点收到主节点数(报错故障节点)/2+1个票数时则进行替换主节点操作

其中每隔配置纪元代表一次选举周期,如果在开始投票后的cluster-node-timeout*2时间内从节点没有获取足够数量的投票,则本次选举作废.从节点对配置纪元自增并发起下一轮投票知道成功.

(5)替换从节点

* slave no one
* 撤销故障主节点的槽并委派给自己
* 向集群广播消息,通知主从状态和槽映射信息

### ==⭐七、集群运维==

#### 1.集群读写分离

##### ①、只读连接

**集群模式下的从节点默认不接受任何读写请求，发送过来的键命令会重定向到负责槽的主节点上。**当需要从节点分担主节点读压力时，可以在客户端中在每次新建连接时都向从节点发送`readonly`命令以开启从节点的读操作**(此命令是连接级别生效,==而主从复制配置的`slave-read-only`在集群模式下是无效的==)**,之后 该连接的从节点 就可以响应其主节点对应槽的读操作

##### ②、读写分离

Smart客户端不支持集群的读写分离,所以读命令的从节点路由必须自己实现**（想了一下，要想实现就只能在客户端里维护主节点到从节点的映射，通过MOVE重定向错误找到key的目标主节点==（MOVE重定向返回的信息都是主节点信息）==，从而向它的从节点发请求）**。而又由于集群模式不支持`slave-read-only`配置，所以每次连接都要发送`read-only`以使得本连接能够处理读操作。

总而言之，要造大量轮子，不如不用读写分离。